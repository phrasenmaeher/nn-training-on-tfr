{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import typing\n",
    "from tensorflow import keras\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    from keras.api._v2 import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "image_shape = (250, 250, 3)\n",
    "number_of_images = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):  # if value ist tensor\n",
    "        value = value.numpy()  # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def generate_random_full_colour_images(n, image_shape):\n",
    "    red = (255, 0, 0)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (0, 0, 255)\n",
    "    colours = {0: red, 1: green, 2: blue}\n",
    "    generated_images = []\n",
    "    generated_labels = []\n",
    "    for _ in range(n):\n",
    "        label = np.random.randint(0, 3, size=None)\n",
    "        img = Image.new('RGB', image_shape[:-1], colours[label])\n",
    "        generated_images.append(np.asarray(img))\n",
    "        generated_labels.append(label)\n",
    "\n",
    "    return np.asarray(generated_images), np.asarray(generated_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 250, 250, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "images, labels = generate_random_full_colour_images(number_of_images, image_shape=image_shape)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def parse_single_image(image, label):\n",
    "    #define the dictionary -- the structure -- of our single example\n",
    "    data = {\n",
    "        'height': _int64_feature(image.shape[0]),\n",
    "        'width': _int64_feature(image.shape[1]),\n",
    "        'depth': _int64_feature(image.shape[2]),\n",
    "        'raw_image': _bytes_feature(serialize_array(image)),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "\n",
    "    #create an Example, wrapping the single features\n",
    "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def write_images_to_tfr(images, labels, filename: str = \"images\", max_files: int = 10,\n",
    "                        out_dir: str = \"./tfrecords/\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    #determine the number of shards (single TFRecord files) we need:\n",
    "    splits = (len(images) // max_files) + 1  #determine how many tfr shards are needed\n",
    "    if len(images) % max_files == 0:\n",
    "        splits -= 1\n",
    "    print(f\"\\nUsing {splits} shard(s) for {len(images)} files, with up to {max_files} samples per shard\")\n",
    "\n",
    "    file_count = 0\n",
    "\n",
    "    for i in tqdm.tqdm(range(splits)):\n",
    "        current_shard_name = \"{}{}_{}{}.tfrecords\".format(out_dir, i + 1, splits, filename)\n",
    "        writer = tf.io.TFRecordWriter(current_shard_name)\n",
    "\n",
    "        current_shard_count = 0\n",
    "        while current_shard_count < max_files:  #as long as our shard is not full\n",
    "            #get the index of the file that we want to parse now\n",
    "            index = i * max_files + current_shard_count\n",
    "            if index == len(images):  #when we have consumed the whole data, preempt generation\n",
    "                break\n",
    "\n",
    "            current_image = images[index]\n",
    "            current_label = labels[index]\n",
    "\n",
    "            #create the required Example representation\n",
    "            out = parse_single_image(image=current_image, label=current_label)\n",
    "\n",
    "            writer.write(out.SerializeToString())\n",
    "            current_shard_count += 1\n",
    "            file_count += 1\n",
    "\n",
    "        writer.close()\n",
    "    print(f\"\\nWrote {file_count} elements to TFRecord\")\n",
    "    return file_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 10 shard(s) for 200 files, with up to 20 samples per shard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 34.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 200 elements to TFRecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = write_images_to_tfr(images, labels, filename=\"images\", max_files=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def parse_tfr_element(element):\n",
    "    #use the same structure as above; it's kinda an outline of the structure we now want to create\n",
    "    data = {\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'raw_image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    height = content['height']\n",
    "    width = content['width']\n",
    "    depth = content['depth']\n",
    "    label = content['label']\n",
    "    raw_image = content['raw_image']\n",
    "\n",
    "    #get our 'feature'-- our image -- and reshape it appropriately\n",
    "    feature = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
    "    feature = tf.reshape(feature, shape=[height, width, depth])\n",
    "    return (feature, label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def get_dataset(tfr_dir: str = \"/content/\", pattern: str = \"*images.tfrecords\"):\n",
    "    files = glob.glob(os.path.join(tfr_dir, pattern), recursive=False)\n",
    "    print(files)\n",
    "\n",
    "    #create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "    #pass every single feature through our mapping function\n",
    "    dataset = dataset.map(\n",
    "        parse_tfr_element\n",
    "    )\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./tfrecords/3_10images.tfrecords', './tfrecords/10_10images.tfrecords', './tfrecords/5_10images.tfrecords', './tfrecords/8_10images.tfrecords', './tfrecords/6_10images.tfrecords', './tfrecords/7_10images.tfrecords', './tfrecords/4_10images.tfrecords', './tfrecords/9_10images.tfrecords', './tfrecords/2_10images.tfrecords', './tfrecords/1_10images.tfrecords']\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(\"./tfrecords\")\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.map(lambda x, y:(tf.cast(x, tf.float32)/255.0, y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 9 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy())\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")\n",
    "plt.savefig(\"three_color_dataset_sample\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = get_model(image_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = keras.optimizers.Adam()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 12:45:35.987541: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 169ms/step - loss: 0.7281 - sparse_categorical_accuracy: 0.8050\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 0.0249 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 6.4167e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 1.0424e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 9.1223e-07 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x157d271f0>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}